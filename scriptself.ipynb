{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4212e-26e1-4a82-8920-0689c7d60988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the needed libraries\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# Random sleep between 2 to 5 seconds\n",
    "def random_sleep():\n",
    "    time.sleep(random.uniform(2, 5))  \n",
    "\n",
    "def click_load_more(driver):\n",
    "    while True:\n",
    "        try:\n",
    "            # Locate the \"Load More\" button\n",
    "            load_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[@id='storage-site-directory-load-more-link']\"))  \n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more_button)  # Scroll to button\n",
    "            load_more_button.click()  # Click the button\n",
    "            random_sleep()  # Random sleep to avoid being blocked/ethical scraping\n",
    "\n",
    "        except TimeoutException:\n",
    "            break  # Break the loop if the button is not found or not clickable\n",
    "        except NoSuchElementException:\n",
    "            break  # Break if the button is no longer in the DOM\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break  # Break on any other exception\n",
    "#Store Required information         \n",
    "storagecenter=[]\n",
    "storagelocation=[]\n",
    "links_to_follow = []\n",
    "contact_details=[]\n",
    "website_links=[]\n",
    "\n",
    "#Use Selenium to get the full page source\n",
    "def getpagesource():\n",
    "    chrome_options = Options()\n",
    "    # Uncomment this if you want to run in headless mode\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    # Setup ChromeDriver\n",
    "    CHROMEDRIVER_PATH = ChromeDriverManager().install()\n",
    "    driver = webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=chrome_options)\n",
    "\n",
    "    # Define the URL\n",
    "    url = 'https://www.ssauk.com/storage-site-directory.html?q=London&radius=5'\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(6)  # Add implicit wait\n",
    "\n",
    "    # Click \"Load More\" until all content is loaded\n",
    "    click_load_more(driver)\n",
    "\n",
    "    # After clicking \"Load More,\" scrape the content\n",
    "    pagesource = driver.page_source\n",
    "    bs = BeautifulSoup(pagesource, 'html.parser')\n",
    "    driver.quit()  # Ensure the driver is closed\n",
    "    return bs\n",
    "bs=getpagesource() #call the getpagesource function\n",
    "\n",
    "#Extract each store name and its address\n",
    "def get_store_and_address():\n",
    "    h3=bs.find_all('h3', {'class':'article-heading'})\n",
    "    for elem in h3:\n",
    "        storename=elem.get_text()\n",
    "        storagecenter.append(storename)\n",
    "    p=bs.find_all('p', {'class':'article-text'})\n",
    "    if p:\n",
    "        for elem in p:\n",
    "            address=elem.get_text()\n",
    "            storagelocation.append(address)\n",
    "    else:\n",
    "        storagelocation.append(\"No location address\")\n",
    "get_store_and_address() #call the get_store_and_address function\n",
    "\n",
    "# Define the links to crawl\n",
    "def links():\n",
    "    # Find all links with \"storage-site-directory/_storage-site\" in their href\n",
    "    for link in bs.find_all('a', {'href': re.compile(r'^https://www.ssauk.com/storage-site-directory/_storage-site')}):\n",
    "        href = link.get('href')  # Extract the href attribute\n",
    "        # Avoid duplicate entries\n",
    "        if href and href not in links_to_follow:\n",
    "            links_to_follow.append(href)\n",
    "links() #call the links() function\n",
    "\n",
    "#Generate the crawler\n",
    "def crawl():\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    for link in links_to_follow:\n",
    "        time.sleep(2)  # Adding a delay between requests to avoid overloading the server\n",
    "        #Extract phone number\n",
    "        retries = 3\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Request with a timeout\n",
    "                response = requests.get(link, headers=headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    break  # Exit the retry loop on successful request\n",
    "                else:\n",
    "                    print(f\"Error {response.status_code} for {link}\")\n",
    "                    break\n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"Timeout on attempt {attempt + 1} for {link}. Retrying...\")\n",
    "                if attempt == retries - 1:\n",
    "                    print(f\"Failed to load {link} after {retries} attempts.\")\n",
    "        p_tag=soup.find('p')\n",
    "        if p_tag:\n",
    "           tel=p_tag.get_text()\n",
    "           contact_details.append(tel)\n",
    "        else:\n",
    "            contact_details.append(\"no contact given\")\n",
    "        #Extract the Website link\n",
    "        a_tags = soup.find_all('a')\n",
    "        # Flag to check if any 'visit' link is found\n",
    "        found_link = False\n",
    "        # Look for the href values of <a> tags that contain \"visit\" in their text\n",
    "        for link in a_tags:\n",
    "            if link.text and 'visit' in link.text.lower():\n",
    "               web_link = link.get('href')\n",
    "               website_links.append(web_link)\n",
    "               found_link = True  # Set the flag to True if a link is found\n",
    "        # If no link containing 'visit' is found, append 'no web_link'\n",
    "        if not found_link:\n",
    "             website_links.append(\"no web_link given\")\n",
    "crawl() #call the crawl function\n",
    "\n",
    "#Print data in excel format\n",
    "df=pd.DataFrame({'Store Name':storagecenter, 'Store Address':storagelocation, \n",
    "                 'Contact Details':contact_details, 'Website Link':website_links})\n",
    "df.to_excel(r\"E:\\PYTHON- DATA SCIENCE\\Data hub\\Scraping-ssauk2.xlsx\") #Change this to reflect your local storage\n",
    "# Print success message\n",
    "print(\"Data downloaded successfully and stored in E:\\\\PYTHON-DATA SCIENCE\\\\Data hub\\\\Scraping-ssauk2.xlsx\")\n",
    "             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
